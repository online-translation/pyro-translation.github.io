

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Regression &mdash; Pyro实例与教程 0.1.0 文档</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="索引"
              href="genindex.html"/>
        <link rel="search" title="搜索" href="search.html"/>
    <link rel="top" title="Pyro实例与教程 0.1.0 文档" href="index.html"/>
        <link rel="next" title="Deep Markov Model" href="dmm.html"/>
        <link rel="prev" title="Variational Autoencoders" href="vae.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Pyro实例与教程
          

          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Pyro里的模型：从原分布到随机函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Inference in Pyro: From Stochastic Functions to Marginal Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Regression">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Bayesian-Regression">Bayesian Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#random_module()"><code class="docutils literal"><span class="pre">random_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#Model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Guide">Guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Validating-Results">Validating Results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro实例与教程</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">索引</a> &raquo;</li>
        
      <li>Bayesian Regression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/bayesian_regression.ipynb.txt" rel="nofollow"> 源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Bayesian-Regression">
<h1>Bayesian Regression<a class="headerlink" href="#Bayesian-Regression" title="永久链接至标题">¶</a></h1>
<p>Regression is one of the most common and basic supervised learning tasks
in machine learning. Suppose we’re given a dataset <span class="math">\(\mathcal{D}\)</span>
of the form</p>
<div class="math">
\[\mathcal{D}  = \{ (X_i, y_i) \} \qquad \text{for}\qquad i=1,2,...,N\]</div>
<p>The goal of linear regression is to fit a function to the data of the
form:</p>
<div class="math">
\[y = w X + b + \epsilon\]</div>
<p>where <span class="math">\(w\)</span> and <span class="math">\(b\)</span> are learnable parameters and
<span class="math">\(\epsilon\)</span> represents observation noise. Specifically <span class="math">\(w\)</span> is
a matrix of weights and <span class="math">\(b\)</span> is a bias vector.</p>
<p>Let’s first implement linear regression in PyTorch and learn point
estimates for the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. Then we’ll see how
to incorporate uncertainty into our estimates by using Pyro to implement
Bayesian linear regression.</p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="永久链接至标题">¶</a></h2>
<p>As always, let’s begin by importing the modules we’ll need.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>import numpy as np
import torch
import torch.nn as nn

from torch.autograd import Variable

import pyro
from pyro.distributions import Normal
from pyro.infer import SVI
from pyro.optim import Adam
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-1-bb6f0bcc58af&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-intense-fg ansi-bold">import</span> numpy <span class="ansi-green-intense-fg ansi-bold">as</span> np
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span><span class="ansi-green-intense-fg ansi-bold">import</span> torch
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-green-intense-fg ansi-bold">import</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>nn <span class="ansi-green-intense-fg ansi-bold">as</span> nn
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-green-intense-fg ansi-bold">from</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>autograd <span class="ansi-green-intense-fg ansi-bold">import</span> Variable

<span class="ansi-red-fg">ImportError</span>: No module named torch
</pre></div></div>
</div>
</div>
<div class="section" id="Data">
<h2>Data<a class="headerlink" href="#Data" title="永久链接至标题">¶</a></h2>
<p>We’ll generate a toy dataset with one feature and <span class="math">\(w = 3\)</span> and
<span class="math">\(b = 1\)</span> as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>N = 100  # size of toy data
p = 1    # number of features

def build_linear_dataset(N, noise_std=0.1):
    X = np.linspace(-6, 6, num=N)
    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)
    X, y = X.reshape((N, 1)), y.reshape((N, 1))
    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))
    return torch.cat((X, y), 1)
</pre></div>
</div>
</div>
<p>Note that we generate the data with a fixed observation noise
<span class="math">\(\sigma = 0.1\)</span>.</p>
</div>
<div class="section" id="Regression">
<h2>Regression<a class="headerlink" href="#Regression" title="永久链接至标题">¶</a></h2>
<p>Now let’s define our regression model in the form of a neural network.
We’ll use PyTorch’s <code class="docutils literal"><span class="pre">nn.Module</span></code> for this. Our input <span class="math">\(X\)</span> is a
matrix of size <span class="math">\(N \times p\)</span> and our output <span class="math">\(y\)</span> is a vector
of size <span class="math">\(p \times 1\)</span>. The function <code class="docutils literal"><span class="pre">nn.Linear(p,</span> <span class="pre">1)</span></code> defines a
linear transformation of the form <span class="math">\(Xw + b\)</span> where <span class="math">\(w\)</span> is the
weight matrix and <span class="math">\(b\)</span> is the additive bias.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>class RegressionModel(nn.Module):
    def __init__(self, p):
        super(RegressionModel, self).__init__()
        self.linear = nn.Linear(p, 1)

    def forward(self, x):
        return self.linear(x)

regression_model = RegressionModel(p)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-3-bfbcaf70fdf9&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-intense-fg ansi-bold">class</span> RegressionModel<span class="ansi-yellow-intense-fg ansi-bold">(</span>nn<span class="ansi-yellow-intense-fg ansi-bold">.</span>Module<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> __init__<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">,</span> p<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>         super<span class="ansi-yellow-intense-fg ansi-bold">(</span>RegressionModel<span class="ansi-yellow-intense-fg ansi-bold">,</span> self<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>__init__<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>         self<span class="ansi-yellow-intense-fg ansi-bold">.</span>linear <span class="ansi-yellow-intense-fg ansi-bold">=</span> nn<span class="ansi-yellow-intense-fg ansi-bold">.</span>Linear<span class="ansi-yellow-intense-fg ansi-bold">(</span>p<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>

<span class="ansi-red-fg">NameError</span>: name &#39;nn&#39; is not defined
</pre></div></div>
</div>
</div>
<div class="section" id="Training">
<h2>Training<a class="headerlink" href="#Training" title="永久链接至标题">¶</a></h2>
<p>We will use the mean squared error (MSE) as our loss and Adam as our
optimizer. We would like to optimize the parameters of the
<code class="docutils literal"><span class="pre">regression_model</span></code> neural net above. We will use a somewhat large
learning rate of <code class="docutils literal"><span class="pre">0.01</span></code> and run for 500 iterations.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>loss_fn = torch.nn.MSELoss(size_average=False)
optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)
num_iterations = 500

def main():
    data = build_linear_dataset(N, p)
    x_data = data[:, :-1]
    y_data = data[:, -1]
    for j in range(num_iterations):
        # run the model forward on the data
        y_pred = regression_model(x_data)
        # calculate the mse loss
        loss = loss_fn(y_pred, y_data)
        # initialize gradients to zero
        optim.zero_grad()
        # backpropagate
        loss.backward()
        # take a gradient step
        optim.step()
        if (j + 1) % 50 == 0:
            print(&quot;[iteration %04d] loss: %.4f&quot; % (j + 1, loss.data[0]))
    # Inspect learned parameters
    print(&quot;Learned parameters:&quot;)
    for name, param in regression_model.named_parameters():
        print(&quot;%s: %.3f&quot; % (name, param.data.numpy()))

if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-4-1c911e0a9b7a&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>loss_fn <span class="ansi-yellow-intense-fg ansi-bold">=</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>nn<span class="ansi-yellow-intense-fg ansi-bold">.</span>MSELoss<span class="ansi-yellow-intense-fg ansi-bold">(</span>size_average<span class="ansi-yellow-intense-fg ansi-bold">=</span>False<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> optim <span class="ansi-yellow-intense-fg ansi-bold">=</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>optim<span class="ansi-yellow-intense-fg ansi-bold">.</span>Adam<span class="ansi-yellow-intense-fg ansi-bold">(</span>regression_model<span class="ansi-yellow-intense-fg ansi-bold">.</span>parameters<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> lr<span class="ansi-yellow-intense-fg ansi-bold">=</span><span class="ansi-cyan-intense-fg ansi-bold">0.01</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> num_iterations <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-cyan-intense-fg ansi-bold">500</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-green-intense-fg ansi-bold">def</span> main<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>

<span class="ansi-red-fg">NameError</span>: name &#39;torch&#39; is not defined
</pre></div></div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">iteration</span> <span class="mi">0050</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">4405.9824</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0100</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">2636.4561</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0150</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1497.4330</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0200</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">811.6588</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0250</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">429.7031</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0300</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">234.2143</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0350</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">142.6732</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0400</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">103.5445</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0450</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">88.2896</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0500</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">82.8650</span>
<span class="n">Learned</span> <span class="n">parameters</span><span class="p">:</span>
<span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="mf">2.996</span>
<span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span> <span class="mf">1.094</span>
</pre></div>
</div>
<p>Not too bad - you can see that the neural net learned parameters that
were pretty close to the ground truth of <span class="math">\(w = 3, b = 1\)</span>. But how
confident should we be in these point estimates?</p>
<p>Bayesian modeling (see
<a class="reference external" href="http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf">here</a>
for an overview) offers a systematic framework for reasoning about model
uncertainty. Instead of just learning point estimates, we’re going to
learn a <em>distribution</em> over values of the parameters <span class="math">\(w\)</span> and
<span class="math">\(b\)</span> that are consistent with the observed data.</p>
</div>
<div class="section" id="Bayesian-Regression">
<h2>Bayesian Regression<a class="headerlink" href="#Bayesian-Regression" title="永久链接至标题">¶</a></h2>
<p>In order to make our linear regression Bayesian, we need to put priors
on the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. These are distributions that
represent our prior belief about reasonable values for <span class="math">\(w\)</span> and
<span class="math">\(b\)</span> (before observing any data).</p>
<div class="section" id="random_module()">
<h3><code class="docutils literal"><span class="pre">random_module()</span></code><a class="headerlink" href="#random_module()" title="永久链接至标题">¶</a></h3>
<p>In order to do this, we’ll ‘lift’ the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>
to random variables. We can do this in Pyro via <code class="docutils literal"><span class="pre">random_module()</span></code>,
which effectively takes a <code class="docutils literal"><span class="pre">nn.Module</span></code> and turns it into a distribution
over neural networks. Specifically, each parameter in the original
neural net is sampled from the provided prior. This allows us to
repurpose vanilla neural nets for use in the Bayesian setting. For
example:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>mu = Variable(torch.zeros(1, 1))
sigma = Variable(torch.ones(1, 1))
# define a unit normal prior
prior = Normal(mu, sigma)
# overload the parameters in the regression nn with samples from the prior
lifted_module = pyro.random_module(&quot;regression_module&quot;, regression_model, prior)
# sample a nn from the prior
sampled_nn = lifted_module()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-5-c785b77cee84&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>mu <span class="ansi-yellow-intense-fg ansi-bold">=</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>zeros<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> sigma <span class="ansi-yellow-intense-fg ansi-bold">=</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>ones<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-red-intense-fg ansi-bold"># define a unit normal prior</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> prior <span class="ansi-yellow-intense-fg ansi-bold">=</span> Normal<span class="ansi-yellow-intense-fg ansi-bold">(</span>mu<span class="ansi-yellow-intense-fg ansi-bold">,</span> sigma<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-red-intense-fg ansi-bold"># overload the parameters in the regression nn with samples from the prior</span>

<span class="ansi-red-fg">NameError</span>: name &#39;Variable&#39; is not defined
</pre></div></div>
</div>
</div>
<div class="section" id="Model">
<h3>Model<a class="headerlink" href="#Model" title="永久链接至标题">¶</a></h3>
<p>We now have all the ingredients needed to specify our model. First we
define priors over <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. Because we’re uncertain
about the parameters a priori, we’ll use relatively wide priors
<span class="math">\(\mathcal{N}(\mu = 0, \sigma = 10)\)</span>. Then we wrap
<code class="docutils literal"><span class="pre">regression_model</span></code> with <code class="docutils literal"><span class="pre">random_module</span></code> and sample an instance of
the neural net, <code class="docutils literal"><span class="pre">lifted_nn</span></code>. We then run the neural net forward on the
inputs <code class="docutils literal"><span class="pre">x_data</span></code>. Finally we use the <code class="docutils literal"><span class="pre">pyro.observe</span></code> statement to
condition on the observed data <code class="docutils literal"><span class="pre">y_data</span></code>. Note that we use the same
fixed observation</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>def model(data):
    # Create unit normal priors over the parameters
    x_data = data[:, :-1]
    y_data = data[:, -1]
    mu, sigma = Variable(torch.zeros(p, 1)), Variable(10 * torch.ones(p, 1))
    bias_mu, bias_sigma = Variable(torch.zeros(1)), Variable(10 * torch.ones(1))
    w_prior, b_prior = Normal(mu, sigma), Normal(bias_mu, bias_sigma)
    priors = {&#39;linear.weight&#39;: w_prior, &#39;linear.bias&#39;: b_prior}
    # lift module parameters to random variables
    lifted_module = pyro.random_module(&quot;module&quot;, regression_model, priors)
    # sample a nn (which also samples w and b)
    lifted_nn = lifted_module()
    # run the nn forward
    latent = lifted_nn(x_data).squeeze()
    # condition on the observed data
    pyro.observe(&quot;obs&quot;, Normal(latent, Variable(0.1 * torch.ones(data.size(0)))),
                 y_data.squeeze())
</pre></div>
</div>
</div>
</div>
<div class="section" id="Guide">
<h3>Guide<a class="headerlink" href="#Guide" title="永久链接至标题">¶</a></h3>
<p>In order to do inference we’re going to need a guide, i.e.&nbsp;a
parameterized family of distributions over <span class="math">\(w\)</span> and <span class="math">\(b\)</span>.
Writing down a guide will proceed in close analogy to the construction
of our model, with the key difference being that the guide parameters
need to be trainable. To do this we register the guide parameters in the
ParamStore using <code class="docutils literal"><span class="pre">pyro.param()</span></code> and make sure each PyTorch
<code class="docutils literal"><span class="pre">Variable</span></code> has the flag <code class="docutils literal"><span class="pre">requires_grad</span></code> set to <code class="docutils literal"><span class="pre">True</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>softplus = torch.nn.Softplus()

def guide(data):
    # define our variational parameters
    w_mu = Variable(torch.randn(p, 1), requires_grad=True)
    # note that we initialize our sigmas to be pretty narrow
    w_log_sig = Variable(-3.0 * torch.ones(p, 1) + 0.05 * torch.randn(p, 1),
                         requires_grad=True)
    b_mu = Variable(torch.randn(1), requires_grad=True)
    b_log_sig = Variable(-3.0 * torch.ones(1) + 0.05 * torch.randn(1),
                         requires_grad=True)
    # register learnable params in the param store
    mw_param = pyro.param(&quot;guide_mean_weight&quot;, w_mu)
    sw_param = softplus(pyro.param(&quot;guide_log_sigma_weight&quot;, w_log_sig))
    mb_param = pyro.param(&quot;guide_mean_bias&quot;, b_mu)
    sb_param = softplus(pyro.param(&quot;guide_log_sigma_bias&quot;, b_log_sig))
    # guide distributions for w and b
    w_dist, b_dist = Normal(mw_param, sw_param), Normal(mb_param, sb_param)
    dists = {&#39;linear.weight&#39;: w_dist, &#39;linear.bias&#39;: b_dist}
    # overload the parameters in the module with random samples
    # from the guide distributions
    lifted_module = pyro.random_module(&quot;module&quot;, regression_model, dists)
    # sample a nn (which also samples w and b)
    return lifted_module()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-7-e29b981aacc9&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>softplus <span class="ansi-yellow-intense-fg ansi-bold">=</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>nn<span class="ansi-yellow-intense-fg ansi-bold">.</span>Softplus<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-green-intense-fg ansi-bold">def</span> guide<span class="ansi-yellow-intense-fg ansi-bold">(</span>data<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     <span class="ansi-red-intense-fg ansi-bold"># define our variational parameters</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>     w_mu <span class="ansi-yellow-intense-fg ansi-bold">=</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>randn<span class="ansi-yellow-intense-fg ansi-bold">(</span>p<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> requires_grad<span class="ansi-yellow-intense-fg ansi-bold">=</span>True<span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;torch&#39; is not defined
</pre></div></div>
</div>
<p>Note that we choose Gaussians for both guide distributions. Also, to
ensure positivity, we pass each log sigma through a <code class="docutils literal"><span class="pre">softplus()</span></code>
transformation.</p>
</div>
</div>
<div class="section" id="Inference">
<h2>Inference<a class="headerlink" href="#Inference" title="永久链接至标题">¶</a></h2>
<p>To do inference we’ll use stochastic variational inference (SVI) (for an
introduction to SVI, see <a class="reference external" href="svi_part_i">SVI Part I</a>). Just like in the
non-Bayesian linear regression, each iteration in our training loop will
take a gradient step, with the difference being that in this case, we’ll
use the ELBO objective instead of the MSE loss.</p>
<p>The Pyro backend will construct the ELBO objective function for us; this
logic is handled by the <code class="docutils literal"><span class="pre">SVI</span></code> class:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>optim = Adam({&quot;lr&quot;: 0.01})
svi = SVI(model, guide, optim, loss=&quot;ELBO&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-8-0fbc31d1d5ae&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>optim <span class="ansi-yellow-intense-fg ansi-bold">=</span> Adam<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">{</span><span class="ansi-blue-intense-fg ansi-bold">&#34;lr&#34;</span><span class="ansi-yellow-intense-fg ansi-bold">:</span> <span class="ansi-cyan-intense-fg ansi-bold">0.01</span><span class="ansi-yellow-intense-fg ansi-bold">}</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> svi <span class="ansi-yellow-intense-fg ansi-bold">=</span> SVI<span class="ansi-yellow-intense-fg ansi-bold">(</span>model<span class="ansi-yellow-intense-fg ansi-bold">,</span> guide<span class="ansi-yellow-intense-fg ansi-bold">,</span> optim<span class="ansi-yellow-intense-fg ansi-bold">,</span> loss<span class="ansi-yellow-intense-fg ansi-bold">=</span><span class="ansi-blue-intense-fg ansi-bold">&#34;ELBO&#34;</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;Adam&#39; is not defined
</pre></div></div>
</div>
<p>Here <code class="docutils literal"><span class="pre">Adam</span></code> is a thin wrapper around <code class="docutils literal"><span class="pre">torch.optim.Adam</span></code> (see
<a class="reference external" href="svi_part_i#Optimizers">here</a> for a discussion). The complete
training loop is as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>def main():
    pyro.clear_param_store()
    data = build_linear_dataset(N, p)
    for j in range(num_iterations):
        # calculate the loss and take a gradient step
        loss = svi.step(data)
        if j % 100 == 0:
            print(&quot;[iteration %04d] loss: %.4f&quot; % (j + 1, loss / float(N)))

if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-9-9af2597861c3&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span> <span class="ansi-green-intense-fg ansi-bold">if</span> __name__ <span class="ansi-yellow-intense-fg ansi-bold">==</span> <span class="ansi-blue-intense-fg ansi-bold">&#39;__main__&#39;</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">---&gt; 11</span><span class="ansi-red-fg">     </span>main<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-fg">&lt;ipython-input-9-9af2597861c3&gt;</span> in <span class="ansi-cyan-fg">main</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-intense-fg ansi-bold">def</span> main<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg">     </span>pyro<span class="ansi-yellow-intense-fg ansi-bold">.</span>clear_param_store<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     data <span class="ansi-yellow-intense-fg ansi-bold">=</span> build_linear_dataset<span class="ansi-yellow-intense-fg ansi-bold">(</span>N<span class="ansi-yellow-intense-fg ansi-bold">,</span> p<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     <span class="ansi-green-intense-fg ansi-bold">for</span> j <span class="ansi-green-intense-fg ansi-bold">in</span> range<span class="ansi-yellow-intense-fg ansi-bold">(</span>num_iterations<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>         <span class="ansi-red-intense-fg ansi-bold"># calculate the loss and take a gradient step</span>

<span class="ansi-red-fg">NameError</span>: global name &#39;pyro&#39; is not defined
</pre></div></div>
</div>
<p>To take an ELBO gradient step we simply call the <code class="docutils literal"><span class="pre">step</span></code> method of
<code class="docutils literal"><span class="pre">SVI</span></code>. Notice that the <code class="docutils literal"><span class="pre">data</span></code> argument we pass to <code class="docutils literal"><span class="pre">step</span></code> will be
passed to both <code class="docutils literal"><span class="pre">model()</span></code> and <code class="docutils literal"><span class="pre">guide()</span></code>.</p>
</div>
<div class="section" id="Validating-Results">
<h2>Validating Results<a class="headerlink" href="#Validating-Results" title="永久链接至标题">¶</a></h2>
<p>Let’s compare the variational parameters we learned to our previous
result:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>for name in pyro.get_param_store().get_all_param_names():
    print(&quot;[%s]: %.3f&quot; % (name, pyro.param(name).data.numpy()))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-10-a2f1be9cc124&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-intense-fg ansi-bold">for</span> name <span class="ansi-green-intense-fg ansi-bold">in</span> pyro<span class="ansi-yellow-intense-fg ansi-bold">.</span>get_param_store<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>get_all_param_names<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>     <span class="ansi-green-intense-fg ansi-bold">print</span><span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-blue-intense-fg ansi-bold">&#34;[%s]: %.3f&#34;</span> <span class="ansi-yellow-intense-fg ansi-bold">%</span> <span class="ansi-yellow-intense-fg ansi-bold">(</span>name<span class="ansi-yellow-intense-fg ansi-bold">,</span> pyro<span class="ansi-yellow-intense-fg ansi-bold">.</span>param<span class="ansi-yellow-intense-fg ansi-bold">(</span>name<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>data<span class="ansi-yellow-intense-fg ansi-bold">.</span>numpy<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;pyro&#39; is not defined
</pre></div></div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">guide_log_sigma_weight</span><span class="p">]:</span> <span class="o">-</span><span class="mf">3.217</span>
<span class="p">[</span><span class="n">guide_log_sigma_bias</span><span class="p">]:</span> <span class="o">-</span><span class="mf">3.164</span>
<span class="p">[</span><span class="n">guide_mean_weight</span><span class="p">]:</span> <span class="mf">2.966</span>
<span class="p">[</span><span class="n">guide_mean_bias</span><span class="p">]:</span> <span class="mf">0.941</span>
</pre></div>
</div>
<p>As you can see, the means of our parameter estimates are pretty close to
the values we previously learned. Now, however, instead of just point
estimates, the parameters <code class="docutils literal"><span class="pre">guide_log_sigma_weight</span></code> and
<code class="docutils literal"><span class="pre">guide_log_sigma_bias</span></code> provide us with uncertainty estimates. (Note
that the sigmas are in log-space here, so the more negative the value,
the narrower the width).</p>
<p>Finally, let’s evaluate our model by checking its predictive accuracy on
new test data. This is known as <em>point evaluation</em>. We’ll sample 20
neural nets from our posterior and run them on the new test data, then
average across their predictions and calculate the MSE of the predicted
values compared to the ground truth.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>X = np.linspace(6, 7, num=20)
y = 3 * X + 1
X, y = X.reshape((20, 1)), y.reshape((20, 1))
x_data, y_data = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))
loss = nn.MSELoss()
y_preds = Variable(torch.zeros(20, 1))
for i in range(20):
    # guide does not require the data
    sampled_nn = guide(None)
    # run the nn and add to total
    y_preds = y_preds + sampled_nn(x_data)
# take the average of the predictions
y_preds = y_preds / 20
print &quot;Loss: &quot;, loss(y_preds, y_data).data[0]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-11-688e96b7eb6e&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> y <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-cyan-intense-fg ansi-bold">3</span> <span class="ansi-yellow-intense-fg ansi-bold">*</span> X <span class="ansi-yellow-intense-fg ansi-bold">+</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> X<span class="ansi-yellow-intense-fg ansi-bold">,</span> y <span class="ansi-yellow-intense-fg ansi-bold">=</span> X<span class="ansi-yellow-intense-fg ansi-bold">.</span>reshape<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">20</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> y<span class="ansi-yellow-intense-fg ansi-bold">.</span>reshape<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">20</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> </span>x_data<span class="ansi-yellow-intense-fg ansi-bold">,</span> y_data <span class="ansi-yellow-intense-fg ansi-bold">=</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>Tensor<span class="ansi-yellow-intense-fg ansi-bold">(</span>X<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>Tensor<span class="ansi-yellow-intense-fg ansi-bold">(</span>y<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> loss <span class="ansi-yellow-intense-fg ansi-bold">=</span> nn<span class="ansi-yellow-intense-fg ansi-bold">.</span>MSELoss<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> y_preds <span class="ansi-yellow-intense-fg ansi-bold">=</span> Variable<span class="ansi-yellow-intense-fg ansi-bold">(</span>torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>zeros<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">20</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;Variable&#39; is not defined
</pre></div></div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Loss</span><span class="p">:</span>  <span class="mf">0.0310659464449</span>
</pre></div>
</div>
<p>See the full code on
<a class="reference external" href="https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py">Github</a>.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dmm.html" class="btn btn-neutral float-right" title="Deep Markov Model" accesskey="n" rel="next">下一章 <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vae.html" class="btn btn-neutral" title="Variational Autoencoders" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> 上一章</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, 小熊.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/translations.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>